{"name":"ImitationMG","tagline":"Learning in the Minority Game","body":"### Minority Game using Reinforcement, Social (Imitation) Learning.\r\n\r\nMsc Thesis Java implementation. Abstract:\r\nLearning has been given much attention in Artificial Intelligence (AI) and Game Theory (GT) disciplines, as it is the key to intelligent and rational behavior. However in a multiagent setting, as in Multi-agent Systems (MAS), where the environment changes according to the actions of the players, the participants cannot afford to be fully rational and resort to heuristics. In such cases classic Game Theory fails to provide convergence results of the adjustment process, thus losing predictive power. Evolutionary Game Theory (EGT), motivated from biology, has been proven suitable for analyzing bounded rationality and heuristic learning using the robust replicator dynamics. In this thesis we use a famous congestion game with many (odd) participants called the Minority Game (MG) as a learning paradigm. The most critical learning methods used in the MG are reviewed, motivated from both economics and machine learning perspective along with their results. Continuing, individual-reinforcement learning through replicator dynamics is analyzed and the asymptotic properties of the learning procedure in the MG are provided. Moreover, we compare individual learning with social learning through imitation using agent-based simulations. The two types of learning do share common convergence characteristics, but differ in the resource allocation schemes and in terms of robustness. Individual-reinforcement learning is a more utilitarian process maximizing system efficiency with disregard to single-agent performance. On the other hand, social imitation can provide a more egalitarian setting where individual scores are almost equal.  \r\n\r\n### How It Works\r\n\r\nIn this model each square patch is a distinct agent.\r\n\r\nSingle-Population Case (Social Learning):\r\nThere is a single population of agents and each agent is equipped with a mixed strategy to play actions {-1,1}. Each round an odd number of agents (NumberOfAgentsInPlay) are chosen to play the Minority Game. In short, the chosen agents play their encoded strategy and the ones using the action of the minority, win a point. At each revision round(NumberOfImitationRounds ) , agents have the chance to revise their strategy. Each agent (imitator) picks another agent (called agent-reference) at random  from his OWN population. If the agent-reference has a better score than the imitator, the imitator takes the action without mutation following the pairwise revision protocol. \r\nThat is he/she imitates with a probability proportional to their payoff difference.\r\n    \r\nMulti-Population Case (Reinforcement Learning):\r\nThere are an odd number of population, consisting of agents.\r\n\r\nEvery population can be subdivided into two subpopulations of agents using action 1 and 0. Agents using action 1, 0 are colored blue and red respectively. Therefore, each population can be seen as a single agent using the mixed strategy corresponding to the portion of agents playing action 1, 0 respectively. In each round, a random agent from each population is chosen to play the Minority Game. In short, the chosen agents play their encoded action and the ones using the action of the minority, win a point.\r\n\r\nAt each revision round(NumberOfImitationRounds ) , agents have the chance to revise their action. Each agent (imitator) picks another agent (called agent-reference) at random  from his OWN population. If the agent-reference has a better score than the imitator, the imitator takes the action without mutation following the pairwise revision protocol.      \r\n\r\n\r\n### How To Use It\r\n\r\nSelect a seed to start the Random Number Generator.\r\nSet a large number of the number of Rounds for each Game(NumberOfRounds).\r\nSet the number of repetitions of the Game (NumberOfRuns).\r\nChoose number of initial populations. In the case of one population the game is played with social learning. If the populations are more than one, the algorithm will run the Reinforcement Learning process.\r\nSelect NumberOfAgents for each population.\r\nThe variable NumberOfAgentsInPlay  is used in the single population case to determine how many agents play the game at each round.\r\nThe NumberOfImitators is the number of agents revising their strategy-action when they have a revision opportunity.\r\nNumberOfImitationRounds the interval of rounds when imitation is to happen.\r\nThus, every (NumberOfImitationRounds ) rounds, chosen agents have revision opportunity.\r\nProbChangeFactor is the factor multiplied by the normalized payoff difference to regulate the probability of imitation.\r\nCheck out the Prototype also, at  http://grstathisportfolio.wordpress.com/category/artificial-intelligence/\r\n\r\n### Current Status\r\n\r\nThe core algorithm of reinforcement and imitation learning is done. \r\nJava Documentation is updated.\r\n\r\n### TODO\r\n\r\n- Improve GUI.\r\n- Implement Graphics.\r\n- Improve User Experience.\r\n- Introduce Mutation.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}